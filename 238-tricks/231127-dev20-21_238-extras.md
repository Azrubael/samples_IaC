# 2023-11-27    13:04
=====================


* 238 - Extras
--------------
Here is a few extra things which you may or not use in real life, depending on how you're working on a project.

*Taints and Tolerations*
------------------------
# https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints

---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: "example-key"
    operator: "Exists"
    effect: "NoSchedule"



*Resource Management for Pods and Containers*
---------------------------------------------
# https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
When you specify the resource request for containers in a Pod, the kube-scheduler uses this information to decide which node to place the Pod on.

# Container resources example 
---
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"

# So REQUEST is reseving, LIMIT is restricting



*Job*
-----
# https://kubernetes.io/docs/concepts/workloads/controllers/job/
A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created. Suspending a Job will delete its active Pods until the Job is resumed again.

It is similiar to Pod, but Pod is running continuously and Job during a specific period of time.



*Cronjob*
---------
# https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/
A CronJob creates Jobs on a repeating schedule.
CronJob is meant for performing regular scheduled actions such as backups, report generation, and so on.
CronJobs have limitations and idiosyncrasies. For example, in certain circumstances, a single CronJob can create multiple concurrent Jobs.

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure



*DaemonSet*
-----------
# https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/
A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.

Some typical uses of a DaemonSet are:
- running a cluster storage daemon on every node
- running a logs collection daemon on every node
- running a node monitoring daemon on every node

A DaemonSet is similar to ReplicaSet, but whereas ReplicaSet works with Deployment, DaemonSet works with worker nodes or you can even include the master node.
Let's say you have four worker nodes. And if you run a Pod with DaemonSet, you'll have four Pod on each worker node. So it says a DaemonSet ensures that all or some nodes run a copy of a Pod. As nodes added to a cluster, pods are added to them.

`DaemonSet похож на ReplicaSet, но тогда как ReplicaSet работает с развертыванием, DaemonSet работает с рабочими узлами, или вы даже можете включить главный узел.`
`Допустим, у вас есть четыре рабочих узла. А если вы запустите под с помощью DaemonSet, у вас будет четыре пода на каждом рабочем узле. Итак, скажем, что DaemonSet гарантирует, что на всех или некоторых узлах будет запущена копия пода. По мере добавления узлов в кластер к ним добавляются модули.`

# Usually DaemonSet uses for collecting logs and monitoring nodes.
And all it is doing not directly, so you will have some readymade solutions like Prometheus, Grafana or similiar which collect logs and metrics.

    $ kubectl get ds -A
NAMESPACE     NAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system   kube-proxy   1         1         1       1            1           kubernetes.io/os=linux   2d23h


    $ vim 238-daemonset.yml
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      # these tolerations are to have the daemonset runnable on control plane nodes
      # remove them if your control plane nodes should not run pods
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      # it may be desirable to set a high priority class to ensure that a DaemonSet Pod
      # preempts running Pods
      # priorityClassName: important
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log

    $ kubectl apply -f 238-daemonset.yml
daemonset.apps/fluentd-elasticsearch created
    $ kubectl get ds -A
NAMESPACE     NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system   fluentd-elasticsearch   1         1         0       1            0           <none>                   14s
kube-system   kube-proxy              1         1         1       1            1           kubernetes.io/os=linux   2d23h

    $ kubectl get pod -n kube-system
NAME                                     READY   STATUS    RESTARTS         AGE
coredns-5dd5756b68-658gn                 1/1     Running   10 (50m ago)     2d23h
coredns-5dd5756b68-t6dmx                 1/1     Running   10 (50m ago)     2d23h
etcd-docker-desktop                      1/1     Running   10 (50m ago)     2d23h
fluentd-elasticsearch-wfz28              1/1     Running   0                84s     <--- a running pod
kube-apiserver-docker-desktop            1/1     Running   10 (2m49s ago)   2d23h
kube-controller-manager-docker-desktop   1/1     Running   11 (50m ago)     2d23h
kube-proxy-cjwfn                         1/1     Running   10 (50m ago)     2d23h
kube-scheduler-docker-desktop            1/1     Running   10 (50m ago)     2d23h
storage-provisioner                      1/1     Running   19 (2m ago)      2d23h
vpnkit-controller                        1/1     Running   10 (50m ago)     2d23h

    $ kubectl delete -f 238-daemonset.yml
daemonset.apps "fluentd-elasticsearch" deleted

